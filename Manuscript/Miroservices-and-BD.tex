\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{array,etoolbox,multirow}
\usepackage{float}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Conference Paper Title*\\
{\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
should not be used}
\thanks{Identify applicable funding agency here. If none, delete this.}
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
\and
\IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
\IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
\textit{name of organization (of Aff.)}\\
City, Country \\
email address or ORCID}
}

\maketitle

\begin{abstract}
This document is a model and instructions for \LaTeX.
This and the IEEEtran.cls file define the components of your paper [title, text, heads, etc.]. *CRITICAL: Do Not Use Symbols, Special Characters, Footnotes, 
or Math in Paper Title or Abstract.
\end{abstract}

\begin{IEEEkeywords}
component, formatting, style, styling, insert
\end{IEEEkeywords}

\section{Requirements Specification}

Precursor to theorizing about the potential of microservices patterns for big data systems, we need to define what we mean by big data systems and what are the requirements of these systems. System and software requirements come in different flavour and can range from a sketch on a napkin to formal (mathematical) specifications. Therefore, we first need to identify what kind of requirements is the most suitable for the purposes of this study. To answer this question, we first explored the body of evidence to understand the current classification of software requirements. 

There's been various attempts to defining and classifying software and systems requirements. For instance, Sommerville (\cite{sommerville2011software}) classified requirements into three levels of abstraction that are namely 1) user requirements, 2) system requirements and 3) design specifications. The author then mapped these requirements against user acceptance testing, integration testing and unit testing. While this could satisfy the requirements of this study, we opted for a a more general framework provided by Laplante (\cite{laplante2017requirements}). In Laplante's approach, requirements are categorized into three categories of 1) functional requirements, 2) non-functional requirements, and 3) domain requirements. 

Our objective is to define the high-level requirements of big data systems, thus we do not seek to explore 'non-functional' requirements. Non-functional requirements are emerged from the particularities of an environment, such as a banking sector and do not correlate to our study. Therefore, the type of requirements we are looking for is functional and domain requirements.

After clarifying the type of requirements, we then explored the body of evidence to realize the general requirements of big data systems. Indeed, the most discussed characteristics of big data systems are the popular 5Vs which are velocity, veracity, volume, Variety and Value (\cite{Demchenko2014}, \cite{Bughin2016}, \cite{Bahrami2015}, \cite{rad2017big}, \cite{Marz2015}, \cite{Chen2016a} ). Many researchers such as Nadal et al. (\cite{nadal2017software}) have underpinned their artifact development on these characteristics and requirements that emerge from them. 

In an extensive effort, NIST Big Data Public Working Group embarked on a large scale study to extract requirements from variety of application domains such as Healthcare and Life Sciences, Commercial, Energy, Government, and Defense. The result of this study was the formation of general requirements under seven categories. In another effort by Volk et al. (\cite{volk2020identifying}),9 use cases for big data projects are identified by collecting theories and use cases from the literature and categorizing them using a hierarchical clustering algorithm. Bashari et al. (\cite{bashari2016security}) focused on the security and privacy requirements of big data systems, Yu et al. presented the modern components of big data systems \cite{yu2019components}, Eridaputra et al. (\cite{eridaputra2014modeling}) created a generic model for big data requirements using goal oriented approaches, and Al-jaroodi et al. (\cite{al2016characteristics}) investigated general requirements to support big data software development. 

We've also studied the reference architectures developed for big data systems to understand general requirements. In one study, Ataei et al. (\cite{ataei2020big}) assessed the body of evidence and presented with a comprehensive list of big data reference architectures. This study helped us realized the spectrum of big data reference architectures, how they are designed and the general set of requirements.  

By analyzing these studies and by evaluating the design and requirement engineering required for big data reference architectures, we created a set of high-level requirements based on big data characteristics. We have then looked for a rigorous approach to present these requirements. There are numerous approaches used for requirement representation including informal, semiformal and formal methods. For the purposes of this study, we opted for an informal method because it's a well established method in the industry and academia (\cite{kassab2014state}). 

Our approach follows the guidelines explained in ISO/IEC/IEEE standard 29148 for representing functional requirements. Our requirement repesentation is organized in system modes, that is we explain the major components of the system and then describe the requirements. This approach is inspired by the requirement specification expressed for NASA WIRE (wide-field infrared explorer) system explained in \cite{laplante2017requirements}. We also taken inspiration from Software Engineering Body of Knowledge Version (\cite{abran2004software}).

Taking all into consideration, we categorized our requirements based on the major characteristics of big data, that is value, variety, velocity, veracity, and volume (\cite{ataei}), plus  . These requirements are as followings: 

\newcounter{magicrownumbers}
\newcommand\rownumber{\stepcounter{magicrownumbers}\arabic{magicrownumbers}}

\begin{center}
    \begin{table*}
    \renewcommand*{\arraystretch}{1.8}
    \begin{tabular}{ | m{1.2cm} | m{14cm} |}

        \hline

        Volume &

        Vol-1) System needs to support asynchronous, streaming, and batch processing to collect data from centralized, distributed, and cloud data sources, and sensors, instrument and other IOT devices 
        
        Vol-2) System needs to provide a scalable storage for massive data sets 
 
        \\

        \hline

        Velocity & 
        
        Vel-1) System needs to support slow, bursty, and high-throughput data transmission between data sources and computing clusters
        
        Vel-2) System needs to stream data to data consumers in a timely manner 

        Vel-3) System needs to able to ingest multiple, continuous, time varying data streams 

        Vel-4) System shall support fast search from streaming and processed data with high accuracy and relevancy 
        
        Vel-5) System should be able to process data in real-time or near real-time manner 
    
        \\ 

        \hline

        Variety & 

        Var-1) System needs to support data in various formats ranging from structured to semi-structured and unstructured graph, web, text, document, timed, spatial, multimedia, simulation, instrumental, and geo-spatial data. 

        Var-2) System needs to support aggregation, standardization, and normalization of data from disparate sources 

        Var-3) System shall support adaptations mechanisms for schema evolution.

        Var-4) System can provide mechanisms to automatically include new data sources 

        \\

        \hline

        Value & 
        
        Val-1) System needs to able to handle compute-intensive analytical processing and machine learning techniques 
        
        Val-2) System needs to support two types of analytical processing: batch and streaming. 

        Val-3) System needs to support different output file formats for different for reporting and visualizations. 
        
        Val-4) System needs to support streaming results to the consumers 
        
        Val-5) System should support descriptive analytics 

        Val-6) System shall support predictive analytics 

        \\

        \hline

        Security \& Privacy & 
        
        SaP-1) System needs to protect and retain privacy and security of sensitive data.

        SaP-2) System needs to have access control, and multi-level, policy-driven authentication on protected data and processing nodes. 

        \\

        \hline
        
        Veracity &
        
        Ver-1) System needs to support data quality curation including classification, pre-processing, format, reduction, and  transformation. 
        
        Ver-2) System needs to support data provenance including data life cycle management and long-term preservation.
        
        Ver-3) System needs to support data validation in two ways: automatic and human annotated. 

        Ver-4) System should be able to handle data loss or corruption. 
        
        \\

        \hline
  
    \end{tabular}
    \end{table*}
\end{center}


\section{Microservice Patterns}

As a result of this SLR, 50 microservice patterns have bene found. These patterns are then classified based on their function and the problem they solve. Each classifications and it's reasoning is depicted in table \ref{MS-Pattern-Cat}.


\begin{enumerate}
    \item Database per service \checkmark
    \item Shared database \checkmark
    \item Event sourcing \checkmark
    \item Multiple service instances per host \checkmark
    \item API gateway \checkmark
    \item Self registration \checkmark
    \item Service discovery \checkmark
    \item Circuit breaker \checkmark
    \item Bulkhead pattern \checkmark
    \item Command and query responsibility segregation \checkmark
    \item Competing consumers \checkmark
    \item Pipes and filters \checkmark
    \item Strangler \checkmark
    \item Anti-corruption layer \checkmark
    \item External configuration store  \checkmark
    \item Priority queue \checkmark
    \item Log Aggregation \checkmark
    \item Ambassador \checkmark
    \item Sidecar \checkmark
    \item Gateway aggregate \checkmark
    \item Gateway offloading \checkmark
    \item Aggregator \checkmark
    \item Backend for Frontend \checkmark
    \item API Composition \checkmark
    \item Saga transaction management \checkmark
    \item Gateway routing (duplicate) \checkmark
    \item Static content hosting \checkmark
    \item Computer resource consolidation \checkmark
    \item Leader election \checkmark
  \end{enumerate}


  
\begin{center}
    \begin{table*}
    \renewcommand*{\arraystretch}{1.8}
    \begin{tabular}{ | m{3cm} | m{12cm} | }

        \hline

        Category &  Pattern
 
        \\

        \hline

        Data Management &  Database per Service, Shared Database, Event Sourcing, Command and Query Responsibility Segregation
 
        \\

        \hline

        Platform and Infrastructure & Multiple service instances per host, External configuration store, Sidecar, Static content hosting, Computer resource consolidation
 
        \\

        \hline

        Communicational & API gateway,  Anti-corruption layer, Self Registration, Service Discovery, Competing consumers, Pipes and filters, Priority queue, Ambassador, Gateway aggregate, Gateway offloading, Aggregator, Backend for Frontend, API Composition, Saga transaction management, Gateway routing, Leader election
        
 
        \\

        \hline

        Fault Tolerance & Circuit breaker, Bulkhead pattern
 
        \\
        \hline

        Observability & Log Aggregation Pattern
 
        \\
        \hline


    \end{tabular}
    \end{table*}
\end{center}



\begin{center}
    \begin{table*}
    \renewcommand*{\arraystretch}{1.8}
    \begin{tabular}{ | m{2cm} | m{8cm} |  m{2cm} |}

        \hline

        Requirement &  Patterns & Reasoning
 
        \\
        \hline

        Vol-1 &  
        
        1) Database per Service

        2)  Event Sourcing
        
        3)  Command and Query Responsibility Segregation
        
        4)  External Configuration Store
        
        5)  API gateway
        
        6)  Anti-Corruption Layer
        
        7)  Service Discovery
        
        8)  Self Registration
        
        9)  Priority Queue

        10) Gateway Offloading 

        11) Gateway Aggregate 

        12) Leader Election

        13) Log Aggregation Pattern 
        
        & Reasoning 
 
        \\

        \hline

        Vol-2 &  
        
        1) Database per Service  
        
        2) Command and Query Responsibility Segregation
        
        & Reasoning
        
        \\
        \hline

        Vel-1 &  
    
        1) API Gateway 

        2) Service Discovery 

        3) Pipes and Filters 

        4) Gateway Routing 

        5) Leader Election 

        6) Circuit Breaker 

        7) Log Aggregation  
        
        & Reasoning
        
        \\
        \hline

        Vel-2 &  
    
        1) Command and Query Responsibility Segregation
        
        2) API gateway

        3) Competing consumers

        4) Gateway aggregate

        5) Gateway Offloading

        6) Leader Election 
        
        7) Gateway routing 

        & Reasoning

        \\

        \hline

        Vel-3 &  
    
        1) Command and Query Responsibility Segregation
        
        2) API gateway

        3) Competing consumers

        4) Gateway aggregate

        5) Gateway Offloading

        6) Leader Election 
        
        7) Gateway routing 

        & Reasoning
        
        \\
        \hline

        \hline

        Vel-3 &  

        1) API composition
        
        2) API gateway

        4) Gateway aggregate

        5) Gateway Offloading
        
        7) Gateway routing 

        & Reasoning
        
        \\
        \hline

        Vel-4 &  

        1) API composition
        
        2) API gateway

        4) Gateway aggregate

        5) Gateway Offloading
        
        7) Gateway routing 
        
        8) Event Sourcing

        9) Command and Query Responsibility Segregation

        & Reasoning
        
        \\
        \hline

        Vel-5 &  

        1) Leader Election 

        2) Log Aggregation Pattern 

        & Reasoning
        
        \\
        \hline

        
        Var-1 &  
        

        & Reasoning
        
        \\
        \hline

        Var-2 &  
        

        & Reasoning
        
        \\
        \hline
  
    \end{tabular}
    \end{table*}
\end{center}


    
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
